
  Agenda   (7 sessions of 4 hrs each)
  ------------------------------------
    - Big Data
    - Spark - Basics & Architecture 
    - Spark Core API (RDD)
    - RDD Transformations and Actions
    - Shared Variables
    - Spark SQL (DataFrames)
    - Machine Learning Intro & Spark MLlib
    - Introduction to Spark Streaming

  =======================================================

   Big Data
   --------
      
      -> That is huge and complex that can note be stored or processed reasonably well using 
         traditional hardware (single machine based) or traditional data management tools. 

      	1. Volume
	2. Velocity   (real time analytics)
	3. Variety    (unstructured & semi-structured data)
	4. Veracity
	5. Value      (deep analytics)


    
   Computing Cluster
   -----------------

	=> A group of connected nodes whose cumulative resources (disk spacem RAM, CPU cores)
           can be used to distribute storage and processing.	
		-> distributed storage  
		-> distributed processing 

   Hadoop
   ------
	=> Is a framework that provides distributed storage and distributed processing
           solutions for big data processing running on clusters made of commodity hardware.

	 1. HDFS   : Dustributed Storage Framework		
	      	-> Name Node: master daemon (runs on the master node)
	      	-> Data Node: slave daemon  (runs on all slave nodes)

	      	-> data is stored as blocks of 128 MB
	      	-> each block is replicated 3 times (by default)	

                Ex: 10290 MB file  => 128 MB * 80 blocks (10240 MB) + 50MB * 1 block  (81 block)


	 2. YARN  : Cluster Manager
		-> responsible for scheduling of the jobs
		-> allocates resource containers (ex: 3 GB RAM and 3 CPU Cores) for the jobs			
		-> monitors the progress of the jobs

	3. MapReduce : Distributed Processing Framework
		-> Runs several Mapper instances parallely across all the executors 
		  (containers), where each mapper processes a small chunk of data (1 or more blocks)

    MapReduce is not good at:
    ---------------------------
	-> Ad-hoc queries (random-access of data)
	-> Not good with lot of small files
	-> Not good with iterative computations 	
		
		
    What is Spark 
    ------------- 	
	-> Is a unified in-memory distributed computing framework

		-> in-memory computation
			-> Spark can store the intermediate data "in-memory" and can run 
                           subsequent task on these "in-memory" partitions of data.

	-> Spark is written in Scala

	-> Spark Applications can be written in multiple languages.
		-> Scala, Java, Python, R

   Spark Unified Framework
   -----------------------
      -> Provides a set of consistent APIs build on the same execution engine to handle
	 different analytical work-loads	

	Hadoop Eco-System
        -----------------
	Batch Analytics of Unstructured Data	: MapReduce 
	Batch Analytics of Structured Data	: Hive, Impala, Drill, HBase, ..
	Streaming Analytics			: Kafka, Storm, Samza
	Predictive Analytics (using ML)		: Mahout
	Graph Parallel Computations		: Giraph

	Spark Unified Libraries
        ------------------------
	Batch Analytics of Unstructured Data	: Spark Core API
	Batch Analytics of Structured Data	: Spark SQL
	Streaming (real time) Analytics		: Spark Streaming, Structured Streaming
	Predictive Analytics (using ML)		: Spark MLlib
	Graph Parallel Computations		: Spark GraphX

    
   Spark Layered Eco-System
   ------------------------	
	Programming Language  -> Scala, Java, Python, R
	Spark High-Level APIs -> Spark SQL, Spark MLlib, Spark GraphX, Spark Streaming 
	Spark Low Level API   -> Spark-Core API
	Cluster Manager       -> local,  Spark Standalone Scheduler, YARN, Mesos, Kubernetes
	Storage Layer	      -> HDFS, S3, Linux, Windows, RDMBS, NoSQL, Kafka
  

   Spark Architecture
   -------------------

    1. Cluster Manager

	 -> Applications (jobs) are submitted to Cluster Manager
	 -> Schedules athe jobs
	 -> Allocates resources (RAM & CPU Cores) to the jobs
		-> Launches Driver process in one container and various executor containers.

    2. Driver Process
	 -> Master Process
	 -> Contains a "SparkContext" object  (SparkSession in the case of Spark SQL)
	 -> Manages the user code
		-> Analyzes the code
		-> Create physical execution plan
		-> Send the tasks to be executed in the executors.

	Two Modes:  (deploy-mode)
	   -> 1. Client Mode  (default) : Driver runs on the client machine
	   -> 2. Cluster Mode :  Driver runs in one of the container inside the cluster


    3. Executors
	-> Different tasks are sent to executors by the driver process
	-> All tasks perform the same function, but on different partitions
	   (number of tasks = number of partitions)	


    4. SparkContext

	  -> Is the starting point of any Spark Core application 
	     (Note: In the case of SparkSQL, this is "sparkSession" )
	  -> Represents an application
	  -> Acts as a link between the driver and various processes running on the cluster.


   Getting Started with Spark
   --------------------------

    1. You can install Spark locally and work with PySpark shell.
	-> Pre-requisite: Java 8
	-> Download Spark 
		https://spark.apache.org/downloads.html
	-> Download the .tgz file and extract it to a desired folder. 
	-> Setup "SPARK_HOME" env. variable pointing to the installation folder
	-> Also, add %SPARK_HOME%/bin to the PATH env. variable

    2. Using an IDE
	
	-> Popular IDEs  :  Spyder (from Anaconda) 
			    PyCharm
			    Visual Studio Code

	-> Follow the steps mentioned in the shared document to setup PySpark with
	   Juputer Note (or Spyder IDE) - assumeing that you have Anaconda installed.

    3. Databrick Community Edition (Free)
	
	-> https://databricks.com/try-databricks

	-> Signup to the free account
	-> Login
	-> Spend some time going through Quick Start tutorial,
    
  

   RDD (Resilient Distributed Dataset)
   -----------------------------------

      -> Fundamental Data Abstraction of Spark Core API

      -> Represents a distributed collection of in-memory partitions 
	  -> Each partition is a collection of objects.

      -> RDD
	   1. Lineage DAG :  Logical Plan of the RDD maintained by the driver.
	   2. Data	  :  Distributed in-memory partitions


      -> RDDs are immutable
	  -> RDD partitions' content can not modified


      -> RDDs are lazily evaluated
	  -> Transformations only cause Linrage grapgs to be created
	  -> Only action commands trigger executition.

   
   What can we do with an RDD ?
   ----------------------------

      -> Two things:
	
	1. Transformations		
		-> Returns an RDD
		-> Does not cause execution
		-> Causes the creation of LIneage DAG (directed acyclic graph)

	2. Actions
		-> Produces output
		-> Triggers execution


    RDD Lineage DAGs (Logical Plan)
    -------------------------------
	-> An RDD LIneage is a logical plan maintained by the driver that describes
	   how the RDD has to create, It is a graph of all the parent RDDs which causes
	   the creation of this RDD.


   	rdd1 = sc.textFile( filePath )
	   => DAG:  rdd1 -> sc.textFile

	rdd2 = rdd1.map( lambda x: x.upper() )
	   => DAG:  rdd2 -> rdd1.map -> sc.textFile
	
	rdd3 = rdd2.map(lambda x: len(x))
	    => DAG:  rdd3 -> rdd2.map -> rdd1.map -> sc.textFile

	rdd4 = rdd3.filter(lambda x: x > 50)
	    => DAG:  rdd4 -> rdd3.filter -> rdd2.map ->  rdd1.map -> sc.textFile

	rdd4.collect()
	     -> The logical plan (lineage DAG) of the RDD is converted into a physical
		execution plan and a set of tasks to compute that RDD are launched on 
		the cluster (allocated executors) by the driver 

		sc.textFile (rdd1) -> map (rdd2) -> map (rdd3) -> filter (rdd4) -> collect()

	NOTE: The number of tasks launched is equal to the number of partitions of the RDD.


    Creating RDDs
    -------------
	
	There are three ways to create an RDD.

	1. You can create an RDD from external file.

		rdd1 = sc.textFile( <filePath> )
		-> The default number of partitions is based in "sc.defaultMinPartitions" in 
		   the case of sc.textFile

		rddFile = sc.textFile( filePath, 6 )

	2. You can create an RDD from programmatic data

		rdd1 = sc.parallelize( [1,2,3,1,2,4,6,7,8,9,5,6,6,7,8,9,7,0,9] )

		-> The default number of partitions is based in "sc.defaultParallelism" in 
		   the case of sc.parallelize
		-> sc.defaultParallelism = number of cores allocated to the application.
		
		rdd1 = sc.parallelize( [1,2,3,1,2,4,6,7,8,9,5,6,6,7,8,9,7,0,9], 3 )


	3. By applying transformations on existing RDDs, we can create new RDDs

		rdd2 = rdd1.map(lambda x: x*2)


     	NOTE: Use <rdd>.getNumPartitions() to get the partition count of an RDD
		ex: rdd2.getNumPartitions()




   Types of Transformations
   ------------------------

	Two types of transformations:

	1. Narrow
		-> There is no shuffling of data
		-> Partition to partition transformations
		-> Output RDD will have SAME number of partitions as input RDD.


	2. Wide
		-> Causes shuffling of the data
		-> Computation of each partition depends on the data in multiple input partitions
		-> Output RDD may have different number of partitions than input RDD
		-> All wide transformations optionally take a param to control the number of output partitions


   RDD Transformations
   -------------------

    1. map			P: U -> V	
				Element to element transformation
				input RDD: N elements, output RDD: N elements

		rddFile.map(lambda x: len(x)).collect()
		rddFile.map(lambda x: x.split(" ")).collect()

    2. filter			P: U -> Boolean
				Only those elements that return True will be there in the output.
				input RDD: N elements, output RDD: <= N elements

		rddFile.filter(lambda line : len(line) > 51).collect()
		rddFile.filter(lambda line : len( line.split(" ") ) > 8).collect()

   3. glom			P: None
				Transforms n entire partition into one array object
	
		rdd1				rdd2 = rdd1.glom()
	
		P0: 1,2,1,3,2,4,3,5  -> glom -> P0: [1,2,1,3,2,4,3,5]
		P1: 3,2,5,6,7,8,9,8  -> glom -> P1: [3,2,5,6,7,8,9,8]
		P2: 9,8,7,6,2,3,1,0  -> glom -> P2: [9,8,7,6,2,3,1,0]

		rdd1.count() : 24		 rdd2.count() : 3 

    4. flatMap			P: U -> Iterable[V]   (iterable is like a collection)
				Flattens all the iterables produced by the function.


		rddFile.flatMap(lambda x: x.split(" ")).collect()
		rddWords.flatMap(lambda x: x.upper()).collect()
		rddFile.flatMap(lambda x: len(x)).collect()  -> ERROR: not a valid function.


    5. distinct			P: None, Optional: number of partitions
				The output RDD will have distinct lements of the input RDD arranged
				into partitions based on hash partitioning

		rdd1.distinct(4).glom().collect()
		rdd1.distinct().glom().collect()

    6. mapPartitions		P: Iterator[U] -> Iterator[V]
				The function takes the entire partition as input parameter
				Allows you to process at partition level, perform aggregations etc.

		rdd1				rdd2 = rdd1.mapPartitions(lambda x: [ sum(x) ])
	
		P0: 1,2,1,3,2,4,3,5  -> mapPartitions -> P0: [21]
		P1: 3,2,5,6,7,8,9,8  -> mapPartitions -> P1: [..]
		P2: 9,8,7,6,2,3,1,0  -> mapPartitions -> P2: [..]

		rdd1.mapPartitions(lambda x: [sum(x)] ).collect()



    7. mapPartitionsWithIndex	P: Int, Iterator[U] -> Iterator[V]
				
				The function takes the partition-index and entire partition as input parameters.
				Allows you to process at partition level, perform aggregations etc.
		
		rdd1.mapPartitionsWithIndex(lambda pid, data: [(pid, sum(data))] ).collect()






   RDD Actions
   -----------

	1. collect

	2. count







