
  Agenda   (7 sessions of 4 hrs each)
  ------------------------------------
    - Big Data
    - Spark - Basics & Architecture 
    - Spark Core API (RDD)
    - RDD Transformations and Actions
    - Shared Variables
    - Spark SQL (DataFrames)
    - Machine Learning Intro & Spark MLlib
    - Introduction to Spark Streaming

  =======================================================

   Big Data
   --------
      
      -> That is huge and complex that can note be stored or processed reasonably well using 
         traditional hardware (single machine based) or traditional data management tools. 

      	1. Volume
	2. Velocity   (real time analytics)
	3. Variety    (unstructured & semi-structured data)
	4. Veracity
	5. Value      (deep analytics)


    
   Computing Cluster
   -----------------

	=> A group of connected nodes whose cumulative resources (disk spacem RAM, CPU cores)
           can be used to distribute storage and processing.	
		-> distributed storage  
		-> distributed processing 

   Hadoop
   ------
	=> Is a framework that provides distributed storage and distributed processing
           solutions for big data processing running on clusters made of commodity hardware.

	 1. HDFS   : Dustributed Storage Framework		
	      	-> Name Node: master daemon (runs on the master node)
	      	-> Data Node: slave daemon  (runs on all slave nodes)

	      	-> data is stored as blocks of 128 MB
	      	-> each block is replicated 3 times (by default)	

                Ex: 10290 MB file  => 128 MB * 80 blocks (10240 MB) + 50MB * 1 block  (81 block)


	 2. YARN   : Cluster Manager
		-> responsible for scheduling of the jobs
		-> allocates resource containers (ex: 3 GB RAM and 3 CPU Cores) for the jobs			
		-> monitors the progress of the jobs

	3. MapReduce : Distributed Processing Framework
		-> Runs several Mapper instances parallely across all the executors 
		  (containers), where each mapper processes a small chunk of data (1 or more blocks)


    MapReduce is not good at:
    ---------------------------
	-> Ad-hoc queries (random-access of data)
	-> Not good with lot of small files
	-> Not good with iterative computations 	
		
		
    What is Spark 
    ------------- 	
	-> Is a unified in-memory distributed computing framework

		-> in-memory computation
			-> Spark can store the intermediate data "in-memory" and can run 
                           subsequent task on these "in-memory" partitions of data.

	-> Spark is written in Scala

	-> Spark Applications can be written in multiple languages.
		-> Scala, Java, Python, R

   Spark Unified Framework
   -----------------------
      -> Provides a set of consistent APIs build on the same execution engine to handle
	 different analytical work-loads	

	Hadoop Eco-System
        -----------------
	Batch Analytics of Unstructured Data	: MapReduce 
	Batch Analytics of Structured Data	: Hive, Impala, Drill, HBase, ..
	Streaming Analytics			: Kafka, Storm, Samza
	Predictive Analytics (using ML)		: Mahout
	Graph Parallel Computations		: Giraph

	Spark Unified Libraries
        ------------------------
	Batch Analytics of Unstructured Data	: Spark Core API
	Batch Analytics of Structured Data	: Spark SQL
	Streaming (real time) Analytics		: Spark Streaming, Structured Streaming
	Predictive Analytics (using ML)		: Spark MLlib
	Graph Parallel Computations		: Spark GraphX

    
   Spark Layered Eco-System
   ------------------------	
	Programming Language  -> Scala, Java, Python, R
	Spark High-Level APIs -> Spark SQL, Spark MLlib, Spark GraphX, Spark Streaming 
	Spark Low Level API   -> Spark-Core API
	Cluster Manager       -> local,  Spark Standalone Scheduler, YARN, Mesos, Kubernetes
	Storage Layer	      -> HDFS, S3, Linux, Windows, RDMBS, NoSQL, Kafka
  


   Spark Architecture
   -------------------

    1. Cluster Manager

	 -> Applications (jobs) are submitted to Cluster Manager
	 -> Schedules athe jobs
	 -> Allocates resources (RAM & CPU Cores) to the jobs
		-> Launches Driver process in one container and various executor containers.

    2. Driver Process
	 -> Master Process
	 -> Contains a "SparkContext" object  (SparkSession in the case of Spark SQL)
	 -> Manages the user code
		-> Analyzes the code
		-> Create physical execution plan
		-> Send the tasks to be executed in the executors.

	Two Modes:  (deploy-mode)
	   -> 1. Client Mode  (default) : Driver runs on the client machine
	   -> 2. Cluster Mode :  Driver runs in one of the container inside the cluster


    3. Executors
	-> Different tasks are sent to executors by the driver process
	-> All tasks perform the same function, but on different partitions
	   (number of tasks = number of partitions)	


    4. SparkContext

	  -> Is the starting point of any Spark Core application 
	     (Note: In the case of SparkSQL, this is "sparkSession" )
	  -> Represents an application
	  -> Acts as a link between the driver and various processes running on the cluster.


   Getting Started with Spark
   --------------------------

    1. You can install Spark locally and work with PySpark shell.
	-> Pre-requisite: Java 8
	-> Download Spark 
		https://spark.apache.org/downloads.html
	-> Download the .tgz file and extract it to a desired folder. 
	-> Setup "SPARK_HOME" env. variable pointing to the installation folder
	-> Also, add %SPARK_HOME%/bin to the PATH env. variable

    2. Using an IDE
	
	-> Popular IDEs  :  Spyder (from Anaconda) 
			    PyCharm
			    Visual Studio Code

	-> Follow the steps mentioned in the shared document to setup PySpark with
	   Juputer Note (or Spyder IDE) - assumeing that you have Anaconda installed.

    3. Databrick Community Edition (Free)
	
	-> https://databricks.com/try-databricks

	-> Signup to the free account
	-> Login
	-> Spend some time going through Quick Start tutorial,
    
  

   RDD (Resilient Distributed Dataset)
   -----------------------------------

       -> To be discussed ....











